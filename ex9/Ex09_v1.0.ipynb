{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "<div style=\"text-align: right\"> University of Tübingen, Summer Term 2023  &copy; 2023 P. Hennig </div>\n",
    "\n",
    "## Exercise Sheet No. 9 — DL Classifiaction on Binary MNIST\n",
    "\n",
    "---\n",
    "\n",
    "Submission by:\n",
    "\n",
    "* FirstName1, Surname1, Matrikelnummer: MatrikelnummerOfFirstTeamMember\n",
    "* FirstName2, Surname2, Matrikelnummer: MatrikelnummerOfSecondTeamMember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br style=\"margin: 20px\">\n",
    "\n",
    "<!-- neutral: #ededf2 -->\n",
    "<!-- note: #e4eefb -->\n",
    "<!-- warning: #ffe0e0 -->\n",
    "<!-- tip: #e4fae4 -->\n",
    "<div style=\"padding: 10px; border-radius: 10px; border-style: solid; border-width: thin; background-color: #e4eefb; margin: auto; width: 70%; font-size: 20px; text-align: center\">\n",
    "In the lecture, you have seen how to train a neural network. And in past tutorials, you trained a Gaussian Process on binary <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a>. In this tutorial, we will combine this knowledge to train a neural network on binary MNIST, and inspect some of the results.\n",
    "<br style=\"margin: 10px\">    \n",
    "<b>See the <code>Tasks and Evaluation Rules</code> section for more details.</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"margin: 50px\">\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import urllib.request  # to download MNIST\n",
    "import gzip            # to download MNIST\n",
    "from time import time\n",
    "\n",
    "# Numerics\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers as jopt\n",
    "import numpy as np\n",
    "jax.config.update(\"jax_enable_x64\", True)  # use double-precision numbers\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")  # we don't need GPU here\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "from tueplots import bundles\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 200})\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(x_data, y_data, width=1.8, cmap=\"cividis\", title=None):\n",
    "    \"\"\"\n",
    "    Plot all given MNIST images with their corresponding labels.\n",
    "    :param x_data: Numpy array of images with shape ``(b, h, w)``.\n",
    "    :param y_data: Numpy array of labels with shape ``(b,)``\n",
    "    :returns: Figure and axes.\n",
    "    \"\"\"\n",
    "    num_axes = len(x_data)\n",
    "    assert len(y_data) == num_axes, \"Inconsistent inputs!\"\n",
    "    plt.rcParams.update(bundles.beamer_moml(rel_width=width))\n",
    "    fig, axes = plt.subplots(ncols=num_axes)\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(x_data[i], cmap=cmap)\n",
    "        ax.set_title(str(y_data[i]))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Data\n",
    "\n",
    "Since we aim to do binary classification and explore the model confidence, we will focus on two rather similar MNIST handwritten digits: 1 and 7. The following cell contains a convenience class that will allows us to download MNIST, store it persistently, and extract a binarized and standardized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    \"\"\"\n",
    "    Static class to download MNIST into numpy arrays and extract a two-digit\n",
    "    subset.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    X_TRAIN_URL = \"train-images-idx3-ubyte.gz\"\n",
    "    Y_TRAIN_URL = \"train-labels-idx1-ubyte.gz\"\n",
    "    X_TEST_URL = \"t10k-images-idx3-ubyte.gz\"\n",
    "    Y_TEST_URL = \"t10k-labels-idx1-ubyte.gz\"\n",
    "    X_SHAPE = (28, 28)\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls):\n",
    "        \"\"\"\n",
    "        The MNIST dataset used in this notebook has been downloaded with this\n",
    "        function. Returns a dict with the following ``np.uint8`` arrays:\n",
    "        * x_train: (60000, 28, 28), y_train: (60000,)\n",
    "        * x_test:  (10000, 28, 28), y_test:  (10000,)\n",
    "        \"\"\"\n",
    "        x_train = urllib.request.urlopen(cls.BASE_URL + cls.X_TRAIN_URL).read()\n",
    "        x_train = gzip.decompress(x_train)\n",
    "        x_train = np.frombuffer(x_train, np.uint8, offset=16).reshape(\n",
    "            -1, *cls.X_SHAPE)\n",
    "        #\n",
    "        y_train = urllib.request.urlopen(cls.BASE_URL + cls.Y_TRAIN_URL).read()\n",
    "        y_train = gzip.decompress(y_train)\n",
    "        y_train = np.frombuffer(y_train, np.uint8, offset=8)\n",
    "        #\n",
    "        x_test = urllib.request.urlopen(cls.BASE_URL + cls.X_TEST_URL).read()\n",
    "        x_test = gzip.decompress(x_test)\n",
    "        x_test = np.frombuffer(x_test, np.uint8, offset=16).reshape(\n",
    "            -1, *cls.X_SHAPE)\n",
    "        #\n",
    "        y_test = urllib.request.urlopen(cls.BASE_URL + cls.Y_TEST_URL).read()\n",
    "        y_test = gzip.decompress(y_test)\n",
    "        y_test = np.frombuffer(y_test, np.uint8, offset=8)\n",
    "        #\n",
    "        return {\"x_train\": x_train, \"y_train\": y_train,\n",
    "                \"x_test\": x_test, \"y_test\": y_test}\n",
    "\n",
    "    @classmethod\n",
    "    def extract_bmnist(cls, mnist, pos_digit=1, neg_digit=7,\n",
    "                       standardize_imgs=True, dtype=np.float64):\n",
    "        \"\"\"\n",
    "        :param mnist: The output of ``download``\n",
    "        :param standardize_imgs: If true, returned images will have zero mean\n",
    "          and unit variance.\n",
    "        :param dtype: Ideally a large-resolution float.\n",
    "        :returns: A dictionary that is a subset of the given ``mnist``, but\n",
    "          only with ``pos_digit`` labeled as 1, and ``neg_digit`` labeled as 0.\n",
    "        \"\"\"\n",
    "        # gather only desired digits, and label them +1, -1\n",
    "        train_mask = (mnist[\"y_train\"] == pos_digit) | (mnist[\"y_train\"] ==\n",
    "                                                        neg_digit)\n",
    "        test_mask = (mnist[\"y_test\"] == pos_digit) | (mnist[\"y_test\"] ==\n",
    "                                                      neg_digit)\n",
    "        bmnist = {\n",
    "            \"x_train\": mnist[\"x_train\"][train_mask].astype(dtype),\n",
    "            \"y_train\": ((mnist[\"y_train\"][train_mask] == POS_DIGIT)).astype(dtype),\n",
    "            \"x_test\": mnist[\"x_test\"][test_mask].astype(dtype),\n",
    "            \"y_test\": (mnist[\"y_test\"][test_mask] == POS_DIGIT).astype(dtype)}\n",
    "        # sanity check\n",
    "        len_x_train, len_y_train = len(bmnist[\"x_train\"]), len(bmnist[\"y_train\"])\n",
    "        len_x_test, len_y_test = len(bmnist[\"x_test\"]), len(bmnist[\"y_test\"])\n",
    "        assert len_x_train == len_y_train, \"Inconsistent training data in mnist?\"\n",
    "        assert len_x_test == len_y_test, \"Inconsistent test data in mnist?\"\n",
    "        # optionally standardize images\n",
    "        if standardize_imgs:\n",
    "            bmnist[\"x_train\"] -= bmnist[\"x_train\"].reshape(len_x_train, -1).mean(axis=1)[:, None, None]\n",
    "            bmnist[\"x_train\"] /= bmnist[\"x_train\"].reshape(len_x_train, -1).std(axis=1)[:, None, None]\n",
    "            bmnist[\"x_test\"] -= bmnist[\"x_test\"].reshape(len_x_test, -1).mean(axis=1)[:, None, None]\n",
    "            bmnist[\"x_test\"] /= bmnist[\"x_test\"].reshape(len_x_test, -1).std(axis=1)[:, None, None]\n",
    "        #\n",
    "        return bmnist\n",
    "\n",
    "\n",
    "# Attempt to recover preexisting mnist. If not preexisting, download anew and save\n",
    "%store -r mnist\n",
    "try:\n",
    "    mnist\n",
    "    print(\"Fetched MNIST from storage!\")\n",
    "except NameError:\n",
    "    print(\"Downloading MNIST...\")\n",
    "    mnist = MNIST.download()\n",
    "    %store mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "POS_DIGIT, NEG_DIGIT = 1, 7  # feel free to play around with these, but stick to (1, 7) for the submission\n",
    "DTYPE = np.float64\n",
    "bmnist = MNIST.extract_bmnist(mnist, POS_DIGIT, NEG_DIGIT, True, DTYPE)\n",
    "\n",
    "inspect_samples = list(range(0, 10))\n",
    "inspect_batch(bmnist[\"x_train\"][inspect_samples], \n",
    "              bmnist[\"y_train\"][inspect_samples])\n",
    "\n",
    "inspect_samples = list(range(10, 20))\n",
    "inspect_batch(bmnist[\"x_train\"][inspect_samples], \n",
    "              bmnist[\"y_train\"][inspect_samples]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Typical Deep Learning Setup:\n",
    "\n",
    "As seen in the lecture slides, a typical DL training setup features the following components:\n",
    "\n",
    "\n",
    "* **Dataloader:** Given is a dataset $\\mathcal{D} = [(x_i, y_i)]_{i=1}^N$ that maps *inputs* $x_i$ to *ground truth targets* $y_i$. We typically work with random subsets called *batches* $\\mathcal{B} \\stackrel{iid}{\\sim} \\mathcal{D}$. A dataloader has the function of providing said batches.\n",
    "* **Model:** A *neural network* $\\hat{y}_i = f(x_i, \\theta)$ (with parameters $\\theta$), typically composed by many nonlinear, simple, parametrized, and differentiable functions called *layers*. It maps an input $x_i$, to a *predicted* output $\\hat{y}_i$.\n",
    "* **Initializer**: Setting the initial state for the model is also a relevant task. For simpler problems, like this one, it suffices to initialize the weights to small noise. \n",
    "* **Objective**: The optimization *objective* in DL typically follows the *Empirical Risk Minimization* paradigm, featuring a *loss function* $\\ell$ that penalizes differences between every $(y_i, \\hat{y}_i)$ prediction-target pair, coupled with an additive *regularizer* $\\rho$ that does not depend on the data. In such cases, the objective $\\mathcal{L}$ (also called *loss function*) has the form $\\mathcal{L}(\\theta) := \\frac{1}{B} \\sum_{i \\in \\{\\mathcal{B}_1, \\dots, \\mathcal{B}_B\\}} \\big\\{ \\ell(y_i, f(x_i, \\theta)) \\big\\} + \\rho(\\theta)$. Note how it only depends on the network parameters $\\theta$.\n",
    "* **Optimizer**: The essence of DL training is to modify the weights $\\theta$ in order to minimize $\\mathcal{L}$, and to do so in a step-wise, batch-wise manner using gradient information (since $f$ is differentiable, we can compute the derivatives of $\\mathcal{L}$ with respect to $\\theta$, which tell us how to slightly modify $\\theta$ in order to reduce $\\mathcal{L}$).  The optimizer is simply a component that has access to $\\theta$ as well as such derivatives, and can update $\\theta$ according to some heuristic (e.g. the *gradient descent* update is $\\theta^{(t+1)} := \\theta^{(t)} - \\eta \\nabla_{\\theta^{(t)}} \\mathcal{L}$ for some *learning rate* $\\eta \\in \\mathbb{R}_{>0}$).\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: #e4eefb; width: 70%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b><code>jax</code> tip:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "One major reason to use software libraries like <code>jax</code> for DL is that they compute the batch gradients automatically; we just need to define the \"forward\" computations using library components. Another advantage is that DL libraries also provide implementations for popular optimizers.\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">\n",
    "\n",
    "\n",
    "# Tasks and Evaluation Rules:\n",
    "\n",
    "In this tutorial, we will adapt the lecture example to MNIST, and analyze some of the obtained results. Specifically, the tasks are:\n",
    "\n",
    "1. Define a training dataloader that provides `(\"x_train\", \"y_train\")` batches, randomly drawn from $\\mathcal{D}$ without replacement.\n",
    "2. Define a two-class, ReLU, Multi-Layer Perceptron (like the one from the lecture) that maps MNIST images into a scalar, with dimensionalities `(784, 256, 64, 1)`.\n",
    "3. Define the objective: Empirical Risk Minimization via cross-entropy loss coupled with *weight decay* (a.k.a. *L2 regularization*).\n",
    "4. Complete the training and evaluation loop.\n",
    "5. Once successfully trained, gather and plot the following data samples from the test set:\n",
    "  * The 5 \"positive\" examples with largest model output (i.e. clear positives)\n",
    "  * The 5 \"negative\" examples with smallest model output (i.e. clear negatives)\n",
    "  * The 5 \"positive\" examples with smallest model output (i.e. confusing positives)\n",
    "  * The 5 \"negative\" examples with largest model output (i.e. confusing negatives)\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: #ffe0e0; width: 85%;  text-align: left; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>TUTORIAL EVALUATION RULES:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<ul>\n",
    "<li>These tasks can be fulfilled with the already imported libraries, and no further libraries should be needed.</li>\n",
    "<li>The cells below provide some scaffolding code that can be optionally used as a starting point (in which case the docstrings can be used as guidance, and the missing bits are signaled via <code>NotImplemented</code>, <code>NotImplementedError</code> and <code>\"TODO\"</code>.</li>\n",
    "<li>The <code>Expected Result</code> cells can be used as a guidance and to showcase correct functionality. In principle, they don't need to be modified, but it is allowed.</li>\n",
    "<li>Code can be borrowed from the lectures, previous tutorials and other sources but it must be documented via docstrings and/or comments to show sufficient understanding of its interface and functionality (no blind copypaste allowed).</li>\n",
    "<li>The trained model should surpass an accuracy of 95% after a few seconds on modest hardware.</li>\n",
    "</ul>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 10px\">\n",
    "\n",
    "\n",
    "\n",
    "The following hyperparameters allow to achieve that goal on modest hardware (provided as guidance, feel free to modify them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "# model architecture and initialization\n",
    "LAYER_SIZES = (784, 256, 64, 1)\n",
    "INIT_STDDEV = 0.1\n",
    "CLASSIFICATION_THRESHOLD = 0.5\n",
    "\n",
    "# optimizer/objective\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-12\n",
    "\n",
    "# training protocol\n",
    "NUM_BATCHES = 5000\n",
    "BATCH_SIZE = 25\n",
    "RANDOM_SEED = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "\n",
    "We provide a definition for the validation dataloader, which runs exactly once over the test subset. In contrast, `train_dataloader` should provide as many batches as desired, re-running over the dataset as many times as needed. For each run over the dataset, retrieved samples should be randomly sampled without replacement, and this randomness should be fully controlled via the random key `rng`: Running twice with same `rng` key should lead to same \"random\" batches, and different `rng` keys should lead to different \"random\" batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(bmnist, batch_size=50, rng=jax.random.PRNGKey(12345)):\n",
    "    \"\"\"\n",
    "    Given a binary MNIST dataset, this generator runs infinitely, returning\n",
    "    randomized batches from the training split.\n",
    "    \n",
    "    :param bmnist: Dictionary as returned by ``MNIST.extract_bmnist``\n",
    "    :param rng: If a ``jax```random key is given, use it to shuffle\n",
    "      all entries.\n",
    "    :yields: An input-output pair of numpy arrays ``(x, y)``, where\n",
    "      the first dimension of the arrays equals ``batch_size``,\n",
    "      except for the last batch that may be smaller.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO\")\n",
    "\n",
    "\n",
    "def test_dataloader(bmnist, batch_size=50):\n",
    "    \"\"\"\n",
    "    Given a binary MNIST dataset, this generator runs once over its\n",
    "    test split, in batched manner.\n",
    "    \n",
    "    :param bmnist: Dictionary as returned by ``MNIST.extract_bmnist``\n",
    "    :yields: An input-output pair of numpy arrays ``(x, y)``, where\n",
    "      the first dimension of the arrays equals ``batch_size``,\n",
    "      except for the last batch that may be smaller.\n",
    "    \"\"\"\n",
    "    assert batch_size > 0, \"batch_size <= 0 not supported\"\n",
    "    for i in range(0, len(bmnist[\"x_test\"]), batch_size):\n",
    "        x = bmnist[\"x_test\"][i : (i + batch_size), ...]\n",
    "        y = bmnist[\"y_test\"][i : (i + batch_size), ...]\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect our dataloaders:\n",
    "\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: ##e4fae4; width: 80%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>Expected result:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<b>Running the cell below should plot 2 rows of 10 different random digits from the training set. Each digit should be correctly labeled (positive samples with a 1, negative samples with a 0), and all samples should be different. Using different seeds should lead to different results, whereas repeating seed should lead to same results.</b>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shuffled training dataloader with RANDOM_SEED and inspect\n",
    "train_dl = train_dataloader(bmnist, BATCH_SIZE, rng=jax.random.PRNGKey(RANDOM_SEED))\n",
    "x_batch, y_batch = next(iter(train_dl))\n",
    "inspect_samples = np.arange(10)\n",
    "inspect_batch(x_batch[inspect_samples], y_batch[inspect_samples])\n",
    "\n",
    "# Create a shuffled training dataloader with different seed and inspect\n",
    "train_dl = train_dataloader(bmnist, BATCH_SIZE, rng=jax.random.PRNGKey(RANDOM_SEED + 1))\n",
    "x_batch, y_batch = next(iter(train_dl))\n",
    "inspect_samples = np.arange(10)\n",
    "inspect_batch(x_batch[inspect_samples], y_batch[inspect_samples]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: ##e4fae4; width: 80%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>Expected result:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<b>Running the cell below should plot a row of 10 digits from the test set. Each digit must also be correctly labeled.</b>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a test dataloader and inspect\n",
    "test_dl = test_dataloader(bmnist, BATCH_SIZE)\n",
    "x_batch, y_batch = next(iter(test_dl))\n",
    "inspect_samples = np.arange(10)\n",
    "inspect_batch(x_batch[inspect_samples], y_batch[inspect_samples]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Initialization\n",
    "\n",
    "Define a two-class Multi-Layer Perceptron (like the one from the lecture) that maps MNIST images into a scalar, with dimensionalities `(784, 256, 64, 1)`. This means that it featuers 3 layers: one mapping from 784 dimensions to 256, and so on. Each $i^{th}$ layer contains 2 parameters: a weight and a bias $[w_i, b_i]$, such that $x_{out} = \\sigma(w^T x_{in}) + bias)$, where $\\sigma$ is a nonlinearity (in our case ReLU).\n",
    "\n",
    "It should be using `jax`, in order to leverage automatic differentiation and batching.\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: #e4eefb; width: 85%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b><code>jax</code> tip:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "Unlike other popular DL frameworks, <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions\"><code>jax</code> follows a strictly functional paradigm</a>, most notably meaning that the main building blocks are functions without state or side effects. Such functions expect all the input data to be passed through the (also stateless) function parameters, and all the results to be retrieved through the function results. A pure function will always return the same result if invoked with the same inputs. Not following this paradigm (e.g. by passing stateful computations to jax functions) is generally undefined and can lead to <a href=\"https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#just-in-time-compilation-with-jax\">undesired behaviour</a>. For us, this means that we will be writing functions that typically accept multiple inputs and outputs, and the \"state\" (e.g. current parameter values) will be stored in variables outside of those functions.\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(params, inputs, nonlinearity=jax.nn.relu):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of an MLP, defined using JAX components. Note that\n",
    "    it returns the *logits*. To map logits into predicted scores, a sigmoid\n",
    "    function can be applied.\n",
    "    \n",
    "    :param params: List of pairs in the form ``[(w1, b1), (w2, b2), ...]`` where\n",
    "      ``w_i, b_i`` are the weights and biases for layer ``i``, such that a layer\n",
    "      computes ``outputs = nonlinearity((w_i @ inputs) + b_i)``.\n",
    "    :param inputs: Batch of flattened input images with shape ``(batch, in_shape)``\n",
    "    :returns: A vector of shape ``(batch,)``, containing one logit per input that\n",
    "      should predict the corresponding binary class.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO\")\n",
    "\n",
    "\n",
    "def create_mlp_params(layer_sizes, stddev=0.1, rng=jax.random.PRNGKey(12345)):\n",
    "    \"\"\"\n",
    "    Creates MLP parameters of given sizes and initializes them with Gaussian\n",
    "    noise of zero mean and given standard deviation.\n",
    "    :param layer_sizes: List of integers in the form ``[d1, d2, ...]``,\n",
    "      where each MLP layer maps from ``d_i`` dimensions to ``d_{i+1}``.\n",
    "    :param stddev: Standard deviation of the initial Gaussian noise.\n",
    "    :param rng: ``jax.random.PRNGKey`` to draw noise from.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for m, n in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        rng, rng_b = jax.random.split(rng)\n",
    "        w = jax.random.normal(rng, (m, n)) * stddev\n",
    "        b = jax.random.normal(rng_b, (n,)) * stddev\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "\n",
    "def test_predictions(params, bmnist, batch_size=50, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Helper function to run ``sigmoid(model)`` over the whole test subset\n",
    "    and compute the accuracy.\n",
    "    \n",
    "    :param threshold: Any sigmoid outputs above this number will be consider\n",
    "      positive (i.e. a value of 1), otherwise negative (i.e. a value of 0).\n",
    "    :param bmnist: See ``test_dataloader``.\n",
    "    :param batch_size: See ``test_dataloader``.\n",
    "    :returns: The triple ``(accuracy, logits, targets)``, where \n",
    "      ``accuracy`` is the ratio of correctly classified samples, ``logits``\n",
    "      are the predicted logits following the order provided by\n",
    "      ``test_dataloader``, and ``targets`` are the corresponding ground\n",
    "      truth annotations.\n",
    "    \"\"\"\n",
    "    all_logits = []\n",
    "    targets = []\n",
    "    for x_batch, y_batch in test_dataloader(bmnist, batch_size):\n",
    "        logits = mlp(params, x_batch.reshape(len(x_batch), -1))\n",
    "        all_logits.extend(list(logits))\n",
    "        targets.extend(list(y_batch))\n",
    "    #\n",
    "    predictions = jax.nn.sigmoid(np.array(all_logits)) > threshold\n",
    "    targets = np.array(targets)\n",
    "    accuracy = (predictions == targets).sum() / len(predictions)\n",
    "    return accuracy, all_logits, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the model outputs when forward propagating through some samples.\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: ##e4fae4; width: 80%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>Expected result:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<b>Running the cell below should plot the same row of 10 digits from the test set as before (since the test dataloader is not random), but this time each digit is labeled with a noisy logit returned by the initialized (but not yet trained) MLP. Furthermore, a histogram of the accuracy over the whole test set should be plotted for 100 different random initializations, and the resulting distribution should be bell-shaped and centered around 50%.</b>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader\n",
    "test_dl = test_dataloader(bmnist, BATCH_SIZE)\n",
    "x_batch, y_batch = next(iter(test_dl))\n",
    "# instantiate model\n",
    "mlp_params = create_mlp_params(LAYER_SIZES, INIT_STDDEV, \n",
    "                               jax.random.PRNGKey(RANDOM_SEED))\n",
    "# forward pass\n",
    "predictions = mlp(mlp_params, x_batch.reshape(BATCH_SIZE, -1))\n",
    "# plot 10 samples with their random logits\n",
    "inspect_samples = np.arange(10)\n",
    "inspect_batch(x_batch[inspect_samples], \n",
    "              predictions[inspect_samples].round(decimals=3))\n",
    "\n",
    "# compute test accuracy for 100 different random initializations\n",
    "accuracies = []\n",
    "for i in range(100):\n",
    "    mlpp = create_mlp_params(LAYER_SIZES, INIT_STDDEV, jax.random.PRNGKey(RANDOM_SEED + i))\n",
    "    acc, _, _ = test_predictions(mlpp, bmnist, BATCH_SIZE, CLASSIFICATION_THRESHOLD)\n",
    "    accuracies.append(acc)\n",
    "# plot histogram of accuracies\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(accuracies, bins=20)\n",
    "ax.set_ylabel(\"Number of occurrences\")\n",
    "ax.set_xlabel(\"Test accuracy under random initialization\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Evaluation Metrics\n",
    "\n",
    "In order to train the model using gradients, we must define the objective. Recall the formulation:\n",
    "\n",
    "$\\mathcal{L}(\\theta) := \\frac{1}{B} \\sum_{i \\in \\{\\mathcal{B}_1, \\dots, \\mathcal{B}_B\\}} \\big\\{ \\ell(y_i, f(x_i, \\theta)) \\big\\} + \\rho(\\theta)$\n",
    "\n",
    "Here, the objective is the same as in the lecture: $\\ell$ is the binary cross-entropy, and $\\rho$ is the L2 regularizer on all MLP parameters. Remember that `jax` follows a functional paradigm, where all relevant inputs and outputs must be stated explicitly and all side effects are kept outside of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(params, inputs, targets, l2_reg=0.0):\n",
    "    \"\"\"\n",
    "    :param params: Network parameters. See ``mlp`` docstring.\n",
    "    :param inputs: Batch of network inputs. See ``mlp`` docstring.\n",
    "    :param targets: Batch of ground truth annotations corresponding to ``inputs``,\n",
    "      as provided by the dataloader.\n",
    "    :param l2_reg: Strength of the L2 regularization term, such that\n",
    "      ``result = cross_entropy + (0.5 * l2_reg * l2norm(params)**2)``.\n",
    "    :returns: A single scalar representing the empirical risk plus the L2 \n",
    "      regularizer over the given batch, with respect to the given parameters.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run again the first test batch through the model, but this time we gather the loss and gradients.\n",
    "\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: ##e4fae4; width: 80%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>Expected result:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<b>Running the cell below should plot one image per MLP weight matrix, containing the corresponding gradients (which are also matrices of same shape). The loss value displayed at the top should be a positive float.</b>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader\n",
    "test_dl = test_dataloader(bmnist, BATCH_SIZE)\n",
    "x_batch, y_batch = next(iter(test_dl))\n",
    "\n",
    "# instantiate model and forward+backward pass\n",
    "mlp_params = create_mlp_params(LAYER_SIZES, INIT_STDDEV, \n",
    "                               jax.random.PRNGKey(RANDOM_SEED))\n",
    "loss, grads = jax.value_and_grad(loss_fn)(mlp_params, x_batch.reshape(BATCH_SIZE, -1), \n",
    "                                           y_batch, WEIGHT_DECAY)\n",
    "\n",
    "# plot weight gradients and loss\n",
    "plt.rcParams.update(bundles.beamer_moml(rel_width=1.8, rel_height=1.8))\n",
    "fig, axes = plt.subplots(nrows=len(grads))\n",
    "fig.suptitle(loss)\n",
    "for i, ax in enumerate(axes):\n",
    "    wg = grads[i][0].T\n",
    "    absmax = abs(wg).max()\n",
    "    ax.imshow(wg, cmap=\"bwr\", aspect=\"auto\", vmin=-absmax, vmax=absmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "We can finally put all pieces together to train the neural network. The only missing step is to actually update the parameters $\\theta$ using the gradient information. This is the role of the optimizer (more info [here](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html)). In point `#3` of the cell below, we showcase how this is normally handled using `jax`. Note that we depart slightly from the functional paradigm:\n",
    "\n",
    "<br style=\"margin: 3px\">\n",
    "<div style=\"border-radius: 10px; background-color: #e4eefb; width: 85%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b><code>jax</code> tip:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "For the optimization step, <code>jax</code>  takes a notable exception from the functional paradigm via the so-called <a href=\"https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html\">Just-In-Time (JIT) compilation</a>. The idea is that, if we have a function that is expensive to run but has a \"fixed\" structure, we can speed up its computation substantially by allowing the JIT compiler to create an optimized version of it. The downside is that we lose flexibility: not everything can be JIT-compiled (e.g. `if-else` branching is generally not allowed), and some of the elements used get \"frozen\" during compilation, meaning <i>the function becomes stateful</i> (i.e. changing some Python variables after compilation won't alter the behaviour of already-compiled functions, which can cause some confusion).\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 3px\">\n",
    "\n",
    "For us, this means mostly two things:\n",
    "\n",
    "1. The most expensive operations during DL training are typically the forward and backward comptuation, as well as the parameter update. We would like to bundle those into a single `update` function and and JIT-compile it.\n",
    "2. But this function would basically depend on all other components. For this reason it needs to be defined right before the training loop starts. Also, it can not contain any dynamic structure like if-else branches.\n",
    "\n",
    "\n",
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: ##e4fae4; width: 80%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>Expected result:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<b>Running the cell below should initialize and train our MLP for <code>NUM_BATCHES</code> and converge to less than 0.1 loss and over 90% accuracy after a few seconds. As training progresses, loss should generally decrease and accuracy increase.</b>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataloaders\n",
    "train_dl = train_dataloader(bmnist, BATCH_SIZE, rng=jax.random.PRNGKey(RANDOM_SEED))\n",
    "test_dl = test_dataloader(bmnist, BATCH_SIZE)\n",
    "\n",
    "# 2. Model params\n",
    "mlp_params = create_mlp_params(LAYER_SIZES, INIT_STDDEV, \n",
    "                               jax.random.PRNGKey(RANDOM_SEED))\n",
    "\n",
    "# 3. Optimizer and JIT update step\n",
    "opt_init, opt_update, get_params = jopt.sgd(LEARNING_RATE)\n",
    "opt_state = opt_init(mlp_params)\n",
    "\n",
    "@jax.jit\n",
    "def update(step, opt_state, inputs, targets, l2_reg=0.0):\n",
    "    \"\"\"\n",
    "    In order to speed up computations (not really necessary for small\n",
    "    examples like this one, but crucial for larger DL setups), we \n",
    "    \"bundle\" the forwardprop, backprop and update steps into a single\n",
    "    JIT-able function.\n",
    "    \"\"\"\n",
    "    value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state), \n",
    "                                               inputs, targets, l2_reg)\n",
    "    opt_state = opt_update(step, grads, opt_state)\n",
    "    return value, opt_state\n",
    "\n",
    "\n",
    "# Training loop\n",
    "losses, test_accs = [], []  # we will gather losses and accuracies\n",
    "t0 = time()\n",
    "#\n",
    "for batch_t, (x_batch, y_batch) in enumerate(train_dl, 1):\n",
    "    if batch_t > NUM_BATCHES:\n",
    "        break\n",
    "        \n",
    "    # TODO: compute forwardprop, backprop and update step\n",
    "    loss = NotImplemented\n",
    "    \n",
    "    losses.append(loss)\n",
    "    if batch_t % 200 == 0:\n",
    "        \n",
    "        # TODO: run model on test set and gather accuracy\n",
    "        test_acc = NotImplemented\n",
    "        \n",
    "        print(f\"[step {batch_t:07d}] Loss={loss:5f}, Test accuracy={test_acc:2f}\")\n",
    "        test_accs.append((batch_t, test_acc))\n",
    "#\n",
    "print(\"Elapsed seconds:\", time() - t0)\n",
    "\n",
    "\n",
    "plt.rcParams.update(bundles.beamer_moml(rel_width=1.8, rel_height=1.5))\n",
    "fig, (ax_loss, ax_acc) = plt.subplots(nrows=2)\n",
    "#\n",
    "ax_loss.plot(range(NUM_BATCHES), losses)\n",
    "ax_loss.set_title(\"Loss\")\n",
    "#\n",
    "ax_acc.plot(*zip(*test_accs))\n",
    "_ = ax_acc.set_title(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"margin: 50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Trained Results\n",
    "\n",
    "\n",
    "Once successfully trained, gather and plot the following data samples from the test set:\n",
    "\n",
    "* The 5 \"positive\" examples with largest model output (i.e. clear positives)\n",
    "* The 5 \"negative\" examples with smallest model output (i.e. clear negatives)\n",
    "* The 5 \"positive\" examples with smallest model output (i.e. confusing positives)\n",
    "* The 5 \"negative\" examples with largest model output (i.e. confusing negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all required data\n",
    "test_acc, logits, targets = test_predictions(\n",
    "    get_params(opt_state), bmnist, BATCH_SIZE, CLASSIFICATION_THRESHOLD)\n",
    "predictions = np.asarray(jax.nn.sigmoid(np.array(logits)))\n",
    "assert test_acc > 0.9, \"Low accuracy! Has the model been correctly trained?\"\n",
    "\n",
    "\n",
    "def retrieve_interesting_samples(predictions, targets, num_samples=5):\n",
    "    \"\"\"\n",
    "    :param predictions: Numpy array of ``sigmoid(mlp(x_i))`` floats. \n",
    "    :param targets: Numpy array of ground truth scalars ``y_i`` given in\n",
    "      same order as predictions.\n",
    "    :returns: A dictionary ``{\"posmax\": [idx1, idx2, ...], \"posmin\": [...],\n",
    "      \"negmax\": [...], \"negmin\": [...]}`` with the indexes for the N\n",
    "      labeled \"positive\" examples with largest prediction, the N \"positive\"\n",
    "      examples with smallest model output, the N \"negative\" examples with\n",
    "      largest model output and the N \"negative\" examples with smallest model\n",
    "      output, where N is ``num_samples``.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br style=\"margin: 10px\">\n",
    "<div style=\"border-radius: 10px; background-color: ##e4fae4; width: 80%;  text-align: center; margin: auto;\">\n",
    "  <div style=\"background: rgba(0, 0, 0, 0.05); border-style: solid; border-width: thin; border-radius: 10px; filter: brightness(0.9); padding: 10px; \"><p style=\"\"><b>Expected result:</b></p>\n",
    "  </div>\n",
    "<div style=\"padding: 10px\">\n",
    "<b>Running the cell below should gather indexes for the 4 interesting groups of test samples, as described above. Then, each group should be plotted in its own row, where each row contains all samples of the same class. The \"clear\" rows should depict instances that are clearly identifiable, whereas the \"confusing\" rows should depict examples that present some irregularities.</b>\n",
    "</div>    \n",
    "</div>\n",
    "<br style=\"margin: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather interesting samples\n",
    "interesting = retrieve_interesting_samples(predictions, targets, 5)\n",
    "\n",
    "# Plot \"clear\" examples\n",
    "inspect_batch(bmnist[\"x_test\"][interesting[\"posmax\"]], predictions[interesting[\"posmax\"]],\n",
    "              title=\"Clear positives\")\n",
    "\n",
    "inspect_batch(bmnist[\"x_test\"][interesting[\"negmin\"]], predictions[interesting[\"negmin\"]],\n",
    "              title=\"Clear negatives\")\n",
    "\n",
    "# Plot \"confusing\" examples\n",
    "inspect_batch(bmnist[\"x_test\"][interesting[\"posmin\"]], predictions[interesting[\"posmin\"]],\n",
    "              title=\"Confusing positives\")\n",
    "\n",
    "inspect_batch(bmnist[\"x_test\"][interesting[\"negmax\"]], predictions[interesting[\"negmax\"]],\n",
    "              title=\"Confusing negatives\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "End of `Exercise Sheet No. 9 — DL Classifiaction on Binary MNIST`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pn23",
   "language": "python",
   "name": "pn23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
